{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95bd2381-8e37-4a90-8725-cc9821d4f6aa",
      "metadata": {},
      "source": [
        "# ECON526: Assignment 3\n",
        "\n",
        "Jesse Perla, Paul Schrimpf, and Phil Solamine\n",
        "\n",
        "## Setup\n",
        "\n",
        "Feel free to use the following packages (and we have added a few\n",
        "convenience imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "1a25d779",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from numpy.linalg import cond, matrix_rank, norm\n",
        "from scipy.linalg import inv, solve, det, eig, lu, eigvals\n",
        "from scipy.linalg import solve_triangular, eigvalsh, cholesky"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa82d1d-0d42-43b2-bb8c-a4376bbf6b5d",
      "metadata": {},
      "source": [
        "## Q1.1\n",
        "\n",
        "Prove that if you have an eigendecomposition of $A = Q \\Lambda Q^{-1}$,\n",
        "then $A \\cdot A = A^2 = Q \\Lambda^2 Q^{-1}$ where $\\Lambda^2$ is the\n",
        "componentwise square of the eigenvalues. Hint: use the fact that the $Q$\n",
        "is orthonormal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61130622-d498-4d2a-90bd-f2a45eadde1f",
      "metadata": {},
      "source": [
        "(double click to edit your answer)\n",
        "\n",
        "$A \\cdot A = Q \\Lambda Q^{-1} Q \\Lambda Q^{-1}$ \n",
        "\n",
        "$A \\cdot A = Q \\Lambda I \\Lambda Q^{-1}$\n",
        "\n",
        "$A \\cdot A = Q \\Lambda^2 Q^{-1}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa20eaa6-0327-49f3-8b7b-59df743cb361",
      "metadata": {},
      "source": [
        "## Q1.2\n",
        "\n",
        "There is nothing special in part 1.1 about taking the square. Just as\n",
        "you would define the square root of a number $x$ as a $x^{1/2}$ such\n",
        "that $x^{1/2} \\times x^{1/2} = x$, we can do the same thing with a\n",
        "matrix square root $A^{1/2}$ is such that $A^{1/2} \\cdot A^{1/2} = A$.\n",
        "\n",
        "Use the eigendecomposition and take inspiration from part Q1.1 to find\n",
        "$A^{1/2}$ for the $A$ given below. Verify that\n",
        "$A^{1/2} \\cdot A^{1/2} \\approx A$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "396e03bf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.+0.j, 1.+0.j],\n",
              "       [1.+0.j, 2.+0.j]])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "A = np.array([[2, 1], [1, 2]])\n",
        "# modify here to calculate A^{1/2}, i.e. where A^{1/2} A^{1/2} = A\n",
        "Lamda , Q = eig(A)\n",
        "Lamda = np.diag(Lamda)\n",
        "A_sqrt = Q @ np.sqrt(Lamda) @ inv(Q)\n",
        "\n",
        "A_sqrt @ A_sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd2a92a-cab3-4e38-930b-e100560cf144",
      "metadata": {},
      "source": [
        "## Q1.3\n",
        "\n",
        "Consider that both $(-2)\\times(-2) = 2 \\times 2 = 4$. There may be\n",
        "multiple square roots of a matrix, and many matrices do not have a\n",
        "square root. For example, For example, the matrix\n",
        "$A = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}$ does not have a\n",
        "square root.\n",
        "\n",
        "Take this matrix and mention a property of it that suggests why this\n",
        "matrix looks suspicious and does not have a unique square root or why\n",
        "the method might fail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "40ff48f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.+0.j 0.+0.j]\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[0, 1], [0, 0]])\n",
        "# modify here, add comments and an explanation on what the problems here would be\n",
        "Lamda , Q = eig(A)\n",
        "print(Lamda)\n",
        "\n",
        "#Because the matrix A is not full rank, the eigenvalues are not unique."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ae0dee-254a-4f72-98d1-48a9afe94f72",
      "metadata": {},
      "source": [
        "## Q1.4\n",
        "\n",
        "Argue that if a matrix is symmetric positive definite then its square\n",
        "root will also be symmetric, positive definite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800a502e-8881-43de-9a46-4754ea85d5f5",
      "metadata": {},
      "source": [
        "(double click to edit your answer)\n",
        "\n",
        "$A^{1/2} = Q \\Lambda^{1/2} Q^{T}$\n",
        "\n",
        "$A^{1/2} = (Q \\Lambda^{1/2} Q^{T})^T$\n",
        "\n",
        "$A^{1/2} = (A^{1/2})^T$\n",
        "\n",
        "Since all eigenvalues in $\\Lambda$ are positive, all eigenvalues in $\\Lambda^{1/2}$ will also be positive. Therefor, $A^{1/2}$ is positive definite."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "756f264c-bf27-4aaa-b1ea-ea3728f45c0c",
      "metadata": {},
      "source": [
        "## Q2.1\n",
        "\n",
        "Take a random variable with the multivariate normal distribution\n",
        "\n",
        "$$\n",
        "Y \\sim N(\\mu, \\Sigma)\n",
        "$$\n",
        "\n",
        "with $\\mu \\equiv \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix}$ and\n",
        "$\\Sigma \\equiv \\begin{bmatrix} 0.1 & 0.05 \\\\ 0.05 & 0.1 \\end{bmatrix}$\n",
        "\n",
        "Take 10,000 draws using this distribution. Hint: See\n",
        "`np.random.multivariate_normal` in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8956d948",
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 10000\n",
        "# modify here\n",
        "mu = np.array([0,1])\n",
        "Sigma = np.array([[0.1 , 0.05], [0.05, 0.1]])\n",
        "Y_draws = 10000\n",
        "\n",
        "random = np.random.multivariate_normal(mu, Sigma, Y_draws)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993f3550-9e5a-4015-bfa9-10ccaf5ab3f8",
      "metadata": {},
      "source": [
        "## Q2.2\n",
        "\n",
        "Take the `Y_draws` above, calculate the mean and covariance matrix, and\n",
        "compare them to $\\mu$ and $\\Sigma$ to see how close they are. Hint: use\n",
        "`np.cov(Y_draws, rowvar=False)` to get the covariance matrix. What is a\n",
        "natural way to compare how close matrices or vectors are?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "2aae3bd2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.09916962 0.04914868]\n",
            " [0.04914868 0.09967011]]\n",
            "[-4.18487000e-04  1.00152739e+00]\n"
          ]
        }
      ],
      "source": [
        "# modify code here\n",
        "# make sure to state how you are comparing vectors/matrices\n",
        "\n",
        "print(np.cov(random, rowvar=False))\n",
        "print(np.mean(random, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "334884f7-41e3-4fe2-bdbf-33175f1d46ed",
      "metadata": {},
      "source": [
        "**(double click to add an explanation here)**\n",
        "\n",
        "One way is to compare each individual element of the matrix to the corresponding element of the other matrix. In this case, the covariance and mean of the draws are very close to the true values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "476055fc-e904-495f-909b-542526534126",
      "metadata": {},
      "source": [
        "## Q2.3\n",
        "\n",
        "A key property of Gaussian random variables is that for any\n",
        "$Y \\sim N(\\mu, \\Sigma)$ we can write it as a function of a unit\n",
        "multivariate normal $X \\sim N(0, I)$ and a matrix $A$ such that\n",
        "$Y = \\mu + C X$. This is called the Cholesky decomposition of $\\Sigma$.\n",
        "\n",
        "1.  Find the Cholesky of the covariance matrix $\\Sigma$, $C$. You can\n",
        "    use upper or lower Cholesky, but be consistent\n",
        "2.  Draw the same $N$ number of draws from $X \\sim N(0,I)$ as the\n",
        "    previous parts of Q2\n",
        "3.  Transform those draws $Y$\n",
        "4.  Compare the mean and covariance of those transformed $Y$ to $\\mu$\n",
        "    and $\\Sigma$ to see how well the transformation did"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "862530eb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-5.51848679e-04  1.00019021e+00]\n",
            "[[0.12559936 0.04371376]\n",
            " [0.04371376 0.07647029]]\n"
          ]
        }
      ],
      "source": [
        "# modify code here\n",
        "mu_new = np.array([0 , 0])\n",
        "Sigma_new = np.array([[1 , 0], [0, 1]])\n",
        "cholesky = np.linalg.cholesky(Sigma)\n",
        "random_new = np.random.multivariate_normal(mu_new, Sigma_new, Y_draws)\n",
        "\n",
        "y = random_new @ cholesky + mu\n",
        "print(np.mean(y, axis=0))\n",
        "print(np.cov(y, rowvar=False))\n",
        "\n",
        "#The mean is roughly the same, but the covariance matrix is different."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1824992-6abb-4fe5-94d0-bd52d6d9c6d8",
      "metadata": {},
      "source": [
        "## Q2.4\n",
        "\n",
        "The same transformation works with the matrix square root. That is, if\n",
        "$Y \\sim N(\\mu, \\Sigma)$, then $Y = \\mu + \\Sigma^{1/2} X$ where\n",
        "$X \\sim N(0, I)$ where $\\Sigma^{1/2}$ is the matrix square root of\n",
        "$\\Sigma$.\n",
        "\n",
        "Repeat the Q2.3 using this transformation instead. That is,\n",
        "\n",
        "1.  Find the square root of the covariance matrix $\\Sigma$,\n",
        "    $\\Sigma^{1/2}$. Hint: You can do this yourself using previous\n",
        "    sections, use the `scipy` function `scipy.linalg.sqrtm` or use the\n",
        "    scipy function `scipy.linalg.matrix_power` with the argument `0.5`.\n",
        "2.  Draw the same $N$ number of draws from $X \\sim N(0,I)$ as the\n",
        "    previous parts of Q2\n",
        "3.  Transform those draws to form $Y$ samples\n",
        "4.  Compare the mean and covariance of those transformed $Y$ to $\\mu$\n",
        "    and $\\Sigma$ to see how well the transformation did"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "94343e48",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.0013267  1.0023097]\n",
            "[[0.10252995 0.05208497]\n",
            " [0.05208497 0.09994235]]\n"
          ]
        }
      ],
      "source": [
        "# modify code here\n",
        "cov_sqrt = scipy.linalg.sqrtm(Sigma)\n",
        "random_last = np.random.multivariate_normal(mu_new, Sigma_new, Y_draws)\n",
        "y_last = random_last @ cov_sqrt + mu\n",
        "\n",
        "print(np.mean(y_last, axis=0))\n",
        "print(np.cov(y_last, rowvar=False))\n",
        "\n",
        "#The mean is roughly the same, the covariance matrix is also roughly the same."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c79bad4-d2b8-452b-94b9-7fba6528d079",
      "metadata": {},
      "source": [
        "## Q3.1\n",
        "\n",
        "Take a new mean $\\mu$ and covariance matrix, $\\Sigma$. You want to do\n",
        "the same thing as previous examples (i.e., find a matrix, $A$ such that\n",
        "$Y = \\mu + A X$ for $X \\sim N(0,I)$).\n",
        "\n",
        "Take the matrix below, its eigenvalues, and the results of calling\n",
        "Cholesky. Explain why you probably shouldnâ€™t be using it in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "7f4a86cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[9.30949336e-01 0.00000000e+00 0.00000000e+00]\n",
            " [7.51920618e-01 3.58057437e-02 0.00000000e+00]\n",
            " [2.14834462e-01 1.43222975e-01 5.83100164e-08]]\n",
            "Eigenvalues(Sigma) = [-9.98809413e-17  1.95131000e-02  1.48048690e+00]\n"
          ]
        }
      ],
      "source": [
        "mu = np.array([0, 1, 1])\n",
        "Sigma = np.array([\n",
        "    [26, 21, 6],\n",
        "    [21, 17, 5],\n",
        "    [6, 5, 2]\n",
        "]) / 30.0\n",
        "C = scipy.linalg.cholesky(Sigma, lower=True)\n",
        "print(C)\n",
        "print(f\"Eigenvalues(Sigma) = {eigvalsh(Sigma)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c64a4e5-ed6d-4fdb-8c16-d0e8d90b508b",
      "metadata": {},
      "source": [
        "**(double click to add an explanation here)**\n",
        "\n",
        "Since there exists one eigenvalue that is very close to 0, the matrix is not positive definite, meaning that the cholesky decomposition will result in numerical errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "171a077e-9e8f-4369-8ecd-767dcb230fcd",
      "metadata": {},
      "source": [
        "## Q3.2\n",
        "\n",
        "The Cholesky is suspicious, but what about the matrix square root?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "68ae5d97",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigma^-1 = \n",
            "[[0.7208559  0.57388853 0.13298645]\n",
            " [0.57388853 0.4659534  0.14214798]\n",
            " [0.13298645 0.14214798 0.16963261]]\n",
            "Sigma^-1 real only = \n",
            "[[0.7208559  0.57388853 0.13298645]\n",
            " [0.57388853 0.4659534  0.14214798]\n",
            " [0.13298645 0.14214798 0.16963261]]\n"
          ]
        }
      ],
      "source": [
        "Sigma_sqrt = scipy.linalg.sqrtm(Sigma)\n",
        "print(f\"Sigma^-1 = \\n{Sigma_sqrt}\")\n",
        "print(f\"Sigma^-1 real only = \\n{np.real(Sigma_sqrt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6df34a-a113-4845-b4ea-2285bf84c51c",
      "metadata": {},
      "source": [
        "Any comments on whether this would be a reasonable way to transform the\n",
        "random variable into a $Y = \\mu + \\Sigma^{-1} X$ for $X \\sim N(0,I)$?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad7d997-253a-45bc-9464-450d12a3389f",
      "metadata": {},
      "source": [
        "**(double click to add an explanation here)**\n",
        "\n",
        "Given the fact that scipy.linalg.sqrtm uses a spectral decomposition, the existance of a 0 eigenvalue will not affect the resulting matrix square root. Therefore, this is a reasonable way to obtain the matrix square root. Hence, this value can be used in the transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184cd01c-9862-4d4f-b3be-d535f8bac525",
      "metadata": {},
      "source": [
        "## Q3.3\n",
        "\n",
        "Your friend suggests trying an SVD of the covariance matrix, which is\n",
        "always defined. Here are the results for $U S V^T = \\Sigma$,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "1786bcca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "U =\n",
            " [[-0.76451208  0.26337697  0.58834841]\n",
            " [-0.61865349 -0.04339637 -0.78446454]\n",
            " [-0.18107771 -0.96371641  0.19611614]]\n",
            "S =\n",
            " [1.48048690e+00 1.95131000e-02 2.83470951e-17]\n",
            "V' =\n",
            " [[-0.76451208 -0.61865349 -0.18107771]\n",
            " [ 0.26337697 -0.04339637 -0.96371641]\n",
            " [ 0.58834841 -0.78446454  0.19611614]]\n"
          ]
        }
      ],
      "source": [
        "U, S, Vh = scipy.linalg.svd(Sigma) # returns V' not V\n",
        "print(f\"U =\\n {U}\")\n",
        "print(f\"S =\\n {S}\")\n",
        "print(f\"V' =\\n {Vh}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c96b439-ecff-4530-b08f-3ab0602cb18a",
      "metadata": {},
      "source": [
        "Your friend then shows you the following"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "419fe423",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape(A) = (3, 2)\n",
            "A =\n",
            "[[-0.93022207  0.03679094]\n",
            " [-0.75274824 -0.00606201]\n",
            " [-0.22032678 -0.13462087]]\n",
            "A @ A.T =\n",
            "[[0.86666667 0.7        0.2       ]\n",
            " [0.7        0.56666667 0.16666667]\n",
            " [0.2        0.16666667 0.06666667]]\n",
            "||A A' - Sigma|| = 1.1664773918553167e-15\n"
          ]
        }
      ],
      "source": [
        "U_truncated = U[:, :2] # truncating to only use the first two columns\n",
        "S_truncated = np.diag(np.sqrt(S[:2])) # first two singular values\n",
        "A = U_truncated @ S_truncated # using first two singular values\n",
        "print(f\"shape(A) = {A.shape}\")\n",
        "print(f\"A =\\n{A}\")\n",
        "print(f\"A @ A.T =\\n{A @ A.T}\")\n",
        "print(f\"||A A' - Sigma|| = {norm(A @ A.T - Sigma)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "720988fe-e53e-4314-aca5-a471d24acd40",
      "metadata": {},
      "source": [
        "Can you explain why this is a great observation and how it might be\n",
        "useful for transforming the random variable $Y = \\mu + A \\hat{X}$ for\n",
        "some unit normal $\\hat{X}$?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffb8ea16-03f8-4902-beb2-aca18fdd5001",
      "metadata": {},
      "source": [
        "**(double click to add an explanation here)**\n",
        "\n",
        "This means that $A$ has preserved the properties of the initial $US$ from the SVD decomposition. Meanins that $A$ can be used to trasnfrom the random variable $Y = \\mu + A \\hat{X}$ for some unit normal $\\hat{X}$.\n",
        "\n",
        "The fact that $A @ A^{T}$ has the same norm as $\\Sigma$ means that it has the same variability as $\\Sigma$. All this while reducing the diemnsionality of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d7cb06-9b9a-40dd-b470-1ed857b39f4a",
      "metadata": {},
      "source": [
        "## Q3.4\n",
        "\n",
        "Finally, your friend uses this to generate a bunch of random values to\n",
        "check if the random variable works relative to the original and compares\n",
        "the sqrt version to the new one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "0102039d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "||mean(Y_draws) - mu|| = 0.0037134248975424175\n",
            "||mean(Y_draws_sqrt) - mu|| = 0.00903844796459746\n",
            "||mean(Y_draws_hat) - mu|| = 0.005684248334399677\n",
            "||cov(Y_draws) - Sigma|| = 0.006626152448963826\n",
            "||cov(Y_draws_sqrt) - Sigma|| = 0.0066134113861411065\n",
            "||cov(Y_draws_hat) - Sigma|| = 0.0006796979412422555\n"
          ]
        }
      ],
      "source": [
        "N = 100000\n",
        "Y_draws = np.random.multivariate_normal(mu,Sigma, size=N) # unit normal\n",
        "X_draws = np.random.multivariate_normal(np.zeros(3), np.eye(3), size=N) # unit normal\n",
        "Y_draws_sqrt = mu + X_draws @ Sigma_sqrt.T # sqrt version\n",
        "X_hat_draws = np.random.multivariate_normal(np.zeros(2), np.eye(2), size=N) # unit normal\n",
        "Y_draws_hat = mu + X_hat_draws @ A.T # Using the A from the SVD\n",
        "print(f\"||mean(Y_draws) - mu|| = {norm(np.mean(Y_draws, axis=0) - mu)}\")\n",
        "print(f\"||mean(Y_draws_sqrt) - mu|| = {norm(np.mean(Y_draws_sqrt, axis=0) - mu)}\")\n",
        "print(f\"||mean(Y_draws_hat) - mu|| = {norm(np.mean(Y_draws_hat, axis=0) - mu)}\")\n",
        "print(f\"||cov(Y_draws) - Sigma|| = {norm(np.cov(Y_draws, rowvar=False) - Sigma)}\")\n",
        "print(f\"||cov(Y_draws_sqrt) - Sigma|| = {norm(np.cov(Y_draws_sqrt, rowvar=False) - Sigma)}\")\n",
        "print(f\"||cov(Y_draws_hat) - Sigma|| = {norm(np.cov(Y_draws_hat, rowvar=False) - Sigma)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "807becdd-4fac-45d5-aa51-fe60c4a737c9",
      "metadata": {},
      "source": [
        "This worked great. In fact, simulating with the new processes seems\n",
        "almost better than directly using the $Y$ but your friend is confused.\n",
        "When they simulated `Y_draws` and `Y_draws_sqrt` they were drawing\n",
        "3-dimensional random normals, but when they did `Y_draws_hat` it was\n",
        "only 2-dimensional. How can this be and still deliver the same results?\n",
        "Explain it to your friend."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efc4f3da-e79c-48bb-9d5e-2c2cde8553eb",
      "metadata": {},
      "source": [
        "**(double click to add an explanation here)**\n",
        "\n",
        "The reason why this works is because the matrix $A$ is a projection matrix. This means that the resulting matrix $\\hat{Y}$ will be a projection of the original $Y$ onto a lower dimensional space. This means that the resulting $\\hat{Y}$ will have the same variability as the original $Y$ but will have a lower dimensionality."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
